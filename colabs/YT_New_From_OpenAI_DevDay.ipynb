{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/underwater/sam-witteveen-llm-tutorials/blob/main/colabs/YT_New_From_OpenAI_DevDay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWzQO2UpFngi"
      },
      "outputs": [],
      "source": [
        "!pip -q install  ipykernel openai==1.1.1 tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SIwYIl_GFsj",
        "outputId": "b3cd810c-8320-4fc6-9e0c-d3e3dd6120c0"
      },
      "outputs": [],
      "source": [
        "!pip show openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCOqndjqGkC-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU_NcKSFF3Eh"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import time\n",
        "import openai\n",
        "import os\n",
        "import requests\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "526_KT1oGThR"
      },
      "source": [
        "## Basics GPT-4-turbo completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PKk2a-WGgW4"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o83r_Z-4GWOR"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What are some of the key events that happened in Janurary 2023?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4-1106-preview\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzyYxVooHb2D",
        "outputId": "1cb9e427-8022-47c8-9e12-f65deeb278b6"
      },
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjtbfNtMnO2M"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What are some of the key events that happened Janurary 2023 for New Zealand Prime Minister Jacinda Ardern?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAsb3TKCnUGP",
        "outputId": "1c7b05a7-1420-42bb-e665-e06cd31ce9c5"
      },
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2zByccXJUJ2"
      },
      "source": [
        "## JSON Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlcrbbTFJcmY",
        "outputId": "df47993c-76d9-4f46-ad77-2a6f1ba3bad3"
      },
      "outputs": [],
      "source": [
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  response_format={\"type\": \"json_object\"},\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful programmer who always returns your answer in JSON.\"},\n",
        "    {\"role\": \"user\", \"content\": \"give me a list of 5 things for grocery shopping. call the list 'groceries'\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "imuLKmg-KNC8",
        "outputId": "c7e4d514-934b-4c44-bf23-ea62d3106d0f"
      },
      "outputs": [],
      "source": [
        "groceries_list = json.loads(completion.choices[0].message.content)\n",
        "print(groceries_list)\n",
        "groceries_list['groceries'][4]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsfZLGdWNrLf"
      },
      "source": [
        "## New Function Calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PGHymSqNqcC",
        "outputId": "f0bfaef3-e1f0-4b1f-f02a-fbff7e9f2e40"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "# Example dummy function hard coded to return the same weather\n",
        "# In production, this could be your backend API or an external API\n",
        "def get_current_weather(location, unit=\"fahrenheit\"):\n",
        "    print('get current weather called')\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    if \"tokyo\" in location.lower():\n",
        "        return json.dumps({\"location\": location, \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
        "    elif \"san francisco\" in location.lower():\n",
        "        return json.dumps({\"location\": location, \"temperature\": \"72\", \"unit\": \"fahrenheit\"})\n",
        "    else:\n",
        "        return json.dumps({\"location\": location, \"temperature\": \"22\", \"unit\": \"celsius\"})\n",
        "\n",
        "\n",
        "def run_conversation():\n",
        "    # Step 1: send the conversation and available functions to the model\n",
        "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Tokyo?\"}]\n",
        "    tools = [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"get_current_weather\",\n",
        "                \"description\": \"Get the current weather in a given location\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"location\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                        },\n",
        "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "                    },\n",
        "                    \"required\": [\"location\"],\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
        "    )\n",
        "    #get the message response\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "\n",
        "\n",
        "    # Step 2: check if the model wanted to call a function\n",
        "    print(\"step 2\")\n",
        "    if tool_calls:\n",
        "\n",
        "        # Step 3: call the function\n",
        "        print(\"step 3\")\n",
        "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "        available_functions = {\n",
        "            \"get_current_weather\": get_current_weather,\n",
        "        }  # only one function in this example, but you can have multiple\n",
        "        messages.append(response_message)  # extend conversation with assistant's reply\n",
        "        # Step 4: send the info for each function call and function response to the model\n",
        "        print(\"step 4\")\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "            print(f\"Tool call {tool_call}\")\n",
        "            function_response = function_to_call(\n",
        "                location=function_args.get(\"location\"),\n",
        "                unit=function_args.get(\"unit\"),\n",
        "            )\n",
        "            messages.append(\n",
        "                {\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"name\": function_name,\n",
        "                    \"content\": function_response,\n",
        "                }\n",
        "            )  # extend conversation with function response\n",
        "        print('step5')\n",
        "        print(messages)\n",
        "        second_response = openai.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-1106\",\n",
        "            messages=messages,\n",
        "        )  # get a new response from the model where it can see the function response\n",
        "        return second_response\n",
        "\n",
        "print(run_conversation())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6-hso_qfO-Gl",
        "outputId": "62947b21-7454-4ccf-af58-cb33f382c086"
      },
      "outputs": [],
      "source": [
        "get_current_weather(\"tokyo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtVBfTV0Ux0F"
      },
      "source": [
        "## DALL-E 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg-YPPVFZFP5"
      },
      "outputs": [],
      "source": [
        "PROMPT = \"a room full of cats all meditating in a circle\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "5bj7q_f3U06c",
        "outputId": "a5595682-85da-4e1c-a860-e93e3eab6375"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.images.generate(\n",
        "  model=\"dall-e-3\",\n",
        "  prompt= PROMPT,\n",
        "  size=\"1024x1024\",\n",
        "  quality=\"standard\",\n",
        "  n=1,\n",
        ")\n",
        "\n",
        "image_url = response.data[0].url\n",
        "image_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "b4FuIb1qU_Ua",
        "outputId": "51fcd9f6-5bed-456d-c15d-8f33a275b395"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from IPython.display import display\n",
        "\n",
        "def display_and_save_image_from_url(url, scale_percent=100, save_name='image.png'):\n",
        "    # Send a GET request to the specified URL to retrieve the image\n",
        "    response = requests.get(url)\n",
        "    # Open the image\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "\n",
        "    # Calculate the new size, as a percentage of the original size\n",
        "    if scale_percent != 100:\n",
        "        width, height = img.size\n",
        "        new_width = int(width * scale_percent / 100)\n",
        "        new_height = int(height * scale_percent / 100)\n",
        "        img = img.resize((new_width, new_height), Image.ANTIALIAS)\n",
        "\n",
        "    # Save the image locally with the given name\n",
        "    img.save(save_name)\n",
        "\n",
        "    # Display the image in the notebook\n",
        "    display(img)\n",
        "\n",
        "# Call the function with the URL, the scale percentage, and the save name you want\n",
        "display_and_save_image_from_url(image_url, scale_percent=50, save_name='meditating_cats.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vKpA1zkPmpS"
      },
      "source": [
        "## GPT-V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIiHK-kKHetk",
        "outputId": "b43e2cf7-355c-450b-ffa4-477fe5c4a6c3"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"/content/meditating_cats.png\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\"\n",
        "}\n",
        "\n",
        "payload = {\n",
        "    \"model\": \"gpt-4-vision-preview\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"Describe whatâ€™s in this image in detail as a story?\"\n",
        "          },\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "            }\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    \"max_tokens\": 300\n",
        "}\n",
        "\n",
        "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhUkj-oMX0aW",
        "outputId": "86828686-808e-430f-e0f8-9d036cdbfeb4"
      },
      "outputs": [],
      "source": [
        "story = response.json()['choices'][0]['message']['content']\n",
        "\n",
        "print(story)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSB01Xx2f__o"
      },
      "source": [
        "## TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "BydMSO8JgA-8",
        "outputId": "1cdd3a0b-03cc-447c-9f92-28737620a53c"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "from pathlib import Path\n",
        "\n",
        "response = client.audio.speech.create(\n",
        "  model=\"tts-1\",\n",
        "  voice=\"onyx\",\n",
        "  input=story\n",
        ")\n",
        "\n",
        "# Define the path where you want to save the file\n",
        "speech_file_path = Path('/content/story.mp3')\n",
        "\n",
        "# Save the response content (binary content of the mp3 file) to the path\n",
        "with open(speech_file_path, 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Play the audio file\n",
        "Audio(speech_file_path, autoplay=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8LDEMZwLeOh"
      },
      "source": [
        "## Deterministic outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "48fd2d4c95ad465090ef97254a4a10d2",
        "deepnote_cell_type": "code",
        "id": "_3bkzS2FIisD"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import pprint\n",
        "import difflib\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo-1106\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "e54e0958be3746d39b6e4c16c59b395a",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 5,
        "execution_start": 1699034108287,
        "id": "xC_Ns1wOIisD",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "async def get_chat_response(system_message: str, user_request: str, seed: int = None):\n",
        "    try:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_request},\n",
        "        ]\n",
        "\n",
        "\n",
        "        client = OpenAI()\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=GPT_MODEL,\n",
        "            messages=messages,\n",
        "            seed=seed,\n",
        "            max_tokens=200,\n",
        "            temperature=0.7,\n",
        "\n",
        "        )\n",
        "\n",
        "        # print(completion.choices[0].message)\n",
        "        # print(completion.system_fingerprint)\n",
        "\n",
        "\n",
        "        response_content = completion.choices[0].message.content #response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        # print(f\"response content: {response_content}\")\n",
        "        system_fingerprint = completion.system_fingerprint\n",
        "        # print(f\"system_fingerprint: {system_fingerprint}\")\n",
        "        prompt_tokens = completion.usage.prompt_tokens #response[\"usage\"][\"prompt_tokens\"]\n",
        "        completion_tokens = (\n",
        "            completion.usage.total_tokens - prompt_tokens\n",
        "        )\n",
        "\n",
        "        table = f\"\"\"\n",
        "        <table>\n",
        "        <tr><th>Response</th><td>{response_content}</td></tr>\n",
        "        <tr><th>System Fingerprint</th><td>{system_fingerprint}</td></tr>\n",
        "        <tr><th>Number of prompt tokens</th><td>{prompt_tokens}</td></tr>\n",
        "        <tr><th>Number of completion tokens</th><td>{completion_tokens}</td></tr>\n",
        "        </table>\n",
        "        \"\"\"\n",
        "        display(HTML(table))\n",
        "\n",
        "        return response_content\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# This function compares two responses and displays the differences in a table.\n",
        "# Deletions are highlighted in red and additions are highlighted in green.\n",
        "# If no differences are found, it prints \"No differences found.\"\n",
        "\n",
        "\n",
        "def compare_responses(previous_response: str, response: str):\n",
        "    d = difflib.Differ()\n",
        "    diff = d.compare(previous_response.splitlines(), response.splitlines())\n",
        "\n",
        "    diff_table = \"<table>\"\n",
        "    diff_exists = False\n",
        "\n",
        "    for line in diff:\n",
        "        if line.startswith(\"- \"):\n",
        "            diff_table += f\"<tr style='color: red;'><td>{line}</td></tr>\"\n",
        "            diff_exists = True\n",
        "        elif line.startswith(\"+ \"):\n",
        "            diff_table += f\"<tr style='color: green;'><td>{line}</td></tr>\"\n",
        "            diff_exists = True\n",
        "        else:\n",
        "            diff_table += f\"<tr><td>{line}</td></tr>\"\n",
        "\n",
        "    diff_table += \"</table>\"\n",
        "\n",
        "    if diff_exists:\n",
        "        display(HTML(diff_table))\n",
        "    else:\n",
        "        print(\"No differences found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "9d09f63309c449e4929364caccfd7065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 964,
        "execution_start": 1699034108745,
        "id": "ccVYjjpKIisD",
        "outputId": "b156ae90-dd8a-411b-c6b0-784632ad68ef",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "topic = \"a journey to Mars\"\n",
        "system_message = \"You are a helpful assistant that generates short stories.\"\n",
        "user_request = f\"Generate a short story about {topic}.\"\n",
        "\n",
        "previous_response = await get_chat_response(\n",
        "    system_message=system_message, user_request=user_request\n",
        ")\n",
        "\n",
        "response = await get_chat_response(\n",
        "    system_message=system_message, user_request=user_request\n",
        ")\n",
        "\n",
        "# The function compare_responses is then called with the two responses as arguments.\n",
        "# This function will compare the two responses and display the differences in a table.\n",
        "# If no differences are found, it will print \"No differences found.\"\n",
        "compare_responses(previous_response, response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "a5754b8ef4074cf7adb479d44bebd97b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "deepnote_cell_type": "code",
        "id": "u3Xqc74dIisE",
        "outputId": "6b9c4a65-b3ec-4a78-ae3a-7fa382da1856"
      },
      "outputs": [],
      "source": [
        "SEED = 123\n",
        "response = await get_chat_response(\n",
        "    system_message=system_message, seed=SEED, user_request=user_request\n",
        ")\n",
        "previous_response = response\n",
        "response = await get_chat_response(\n",
        "    system_message=system_message, seed=SEED, user_request=user_request\n",
        ")\n",
        "\n",
        "compare_responses(previous_response, response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbXG3RaWKNIU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgPJ87FaKNLi"
      },
      "outputs": [],
      "source": [
        "responses = ''\n",
        "for chunk in response:\n",
        "    # print (chunk) # this will print all chunk objects\n",
        "    # notice how we are converting it to string when concatenating\n",
        "    if chunk. choices [0]. delta. content:\n",
        "        text_chunk = chunk.choices [0].delta.content\n",
        "        print (text_chunk, end-\"\", flush-\"true\")\n",
        "        responses += str(text _chunk)\n",
        "# print (responses)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "polyglot_notebook": {
      "kernelInfo": {
        "defaultKernelName": "csharp",
        "items": [
          {
            "aliases": [],
            "name": "csharp"
          }
        ]
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
